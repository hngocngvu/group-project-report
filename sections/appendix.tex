\label{appendix}
\begin{figure}[H]
\centering
\footnotesize
\begin{verbatim}
{
  "model": "unsloth/Qwen2.5-1.5B-Instruct",
  "training_file": [
    "/home/zendragonxxx/grprj/group-project-b3/data/translation/pretrain/data/pretrain-data.jsonl",
    "/home/zendragonxxx/grprj/group-project-b3/data/translation/bsd_ja_en/training/train_full_context.jsonl"
  ],
  "finetuned_model_id": "TheBlindMaster/Qwen2.5-1.5B-Instruct-emergent-finetune-train_full_context-all-full-r64-e1",
  "max_seq_length": 861,
  "load_in_4bit": false,
  "is_peft": true,
  "loss": "sft",
  "target_modules": [
    "q_proj",
    "k_proj",
    "v_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
  "layers_to_transform": null,
  "r": 64,
  "lora_alpha": 128,
  "lora_dropout": 0.0,
  "use_rslora": true,
  "lr_scheduler_type": "cosine",
  "learning_rate": 0.0001,
  "per_device_train_batch_size": 8,
  "gradient_accumulation_steps": 16,
  "warmup_steps": 100,
  "optim": "adamw_8bit",
  "weight_decay": 0.01,
  "epochs": 1,
  "push_to_private": true,
  "merge_before_push": true,
  "train_on_responses_only": true,
  "evaluation_steps": 2000,
  "save_steps": 4000,
  "early_stopping_patience": 5
}
\end{verbatim}
\caption{Training configuration for \textbf{Qwen2.5-1.5B-Instruct} using LoRA}
\end{figure}
