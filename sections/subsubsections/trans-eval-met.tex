For Japanese-to-English translation evaluation, we use COMETScore \cite{rei2022comet} and BERTScore \cite{zhang2020bertscore} as the primary metrics, with chrF \cite{popovic2015chrf} and $chrF++$ \cite{popovic-2017-chrf} as additional metrics for reference.\\
\vspace{2em}

    \textbf{COMETScore}

    COMET (Crosslingual Optimized Metric for Evaluation of Translation) is a neural-based metric that leverages pre-trained multilingual language models to evaluate translation quality. Unlike traditional n-gram-based metrics, COMET learns to predict human judgments by fine-tuning on human evaluation datasets. We use COMET-22 (wmt22-comet-da), the Direct Assessment model from WMT 2022.

    \begin{equation}
        COMET = f_\theta(src, hyp, ref)
    \end{equation}

    Where $f_\theta$ defines a neural model (typically based on XLM-RoBERTa) that encodes the inputs,
$src$ is the source text, $hyp$ is the hypothesis translation and $ref$ is the reference translation.
    
    COMET produces a quality score and has shown strong correlation with human judgments, being particularly effective for evaluating semantic adequacy.\\

    \vspace{2em}

    \textbf{BERTScore}

    BERTScore leverages pre-trained contextual embeddings from BERT to compute similarity between candidate and reference translations. Instead of exact n-gram matching, it computes semantic similarity at the token level. We use the DeBERTa-xlarge-mnli (microsoft/deberta-xlarge-mnli) backbone, which is the gold standard recommended model for BERTScore.

    \begin{equation}
        BERTScore_F = \frac{2 \cdot P_{BERT} \cdot R_{BERT}}{P_{BERT} + R_{BERT}}
    \end{equation}

    Where $P_{BERT}$ is the precision (matching tokens in candidate to reference) and $R_{BERT}$ is the recall (matching tokens in reference to candidate).

    
    Each token is represented by its contextual embedding, and similarity is computed using cosine similarity.\\
    
    \vspace{2em}

    \textbf{Character n-gram F-score (chrF and chrF++)}

    The Character n-gram F-score (chrF) is an evaluation metric that assesses the quality of machine translation by calculating the F-score of character n-gram overlaps between the hypothesis and the reference. Unlike word-level metrics, chrF works at the character level, making it particularly robust for morphologically rich languages and less sensitive to tokenization errors.

    \begin{equation}
        chrF = \frac{(1 + \beta^2) \cdot chrP \cdot chrR}{\beta^2 \cdot chrP + chrR}
    \end{equation}

    Where chrP (Character Precision) is the percentage of character n-grams in the system output that are also present in the reference,
chrR (Character Recall) is the percentage of character n-grams in the reference that are present in the system output and $\beta$ controls the weight given to recall versus precision; typically $\beta=2$ is used (called chrF++) to emphasize recall.