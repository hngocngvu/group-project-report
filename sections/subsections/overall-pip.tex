This section evaluates the fully integrated system on the complete OpenMantra dataset (212 pages). Following the protocol defined in Section \ref{post-yolo}, we assess the system's ability to detect, order, and translate text in a complete workflow. The evaluation uses a strict containment threshold ($\ge 0.8$) to match detected bubble and ground truth and penalizes missed detections with empty strings to ensure robust metrics across downstream task such as OCR and translation.

\begin{table}[H]
\centering
\caption{Detection and layout analysis results}
\label{tab:detection_layout}
\begin{tabular}{l r}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total GT Bubbles        & 1586 \\
Matched Pairs           & 1518 (95.71\%) \\
Missed Detections       & 68 (4.29\%) \\
False Positives         & 92 (5.80\%) \\
\midrule
Kendall's Tau (Ordering) & 0.8859 \\
Exact Match Rate        & 0.6753 \\
\bottomrule
\end{tabular}
\end{table}

The high Kendall's Tau (0.8859) indicates that the system reliably recovers the correct reading flow, which is critical for context-aware translation.

\begin{table}[H]
\centering
\caption{End-to-end translation performance}
%\label{tab:end2end_translation}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Translation models} & \textbf{BLEU} & \textbf{SacreBLEU} & \textbf{chrF} & \textbf{chrF++} & \textbf{BERTScore} & \textbf{COMET} \\
\midrule
Elan bt & 0.046 & 0.0832 & 0.261 & 0.2401 & 0.5542 & 0.641 \\
Qwen2.5-0.5B & 0.0536& 0.0885& 0.2815& 0.2641& 0.5666& 0.6394\\
(finetuned, context\_window = 4) & & & & & & & \\
Qwen2.5-1.5B & 0.0534& 0.0924 & 0.2779 &0.2573& 0.5788&  0.6896\\
(finetuned, context\_window = 2) & & & & & & & \\
\bottomrule
\end{tabular}
}
\end{table}

We benchmarked the fully integrated pipeline using three representative translation models selected from different architectures to demonstrate the trade-offs between model size and quality. Specifically, we evaluated the best-performing configuration for each model family: Mitsua/elan-mt-bt-ja-en (Baseline), Qwen2.5-0.5B-Instruct (Fine-tuned, context\_window=4), and Qwen2.5-1.5B-Instruct (Fine-tuned, context\_window=2).
Both fine-tuned Qwen models outperformed the baseline Mitsua/elan NMT model across traditional lexical metrics (BLEU, SacreBLEU, and chrF). This confirms that fine-tuning general-purpose LLMs with previous-panel context allows them to generate translations that are structurally closer to the human references than the single-sentence NMT approach.

\begin{table}[H]
\centering
\caption{Ablation Study: Impact of Input Quality (OCR vs. Ground Truth)}
\label{tab:ablation_ocr}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{BLEU}} & \multicolumn{2}{c}{\textbf{COMET}} & \multicolumn{2}{c}{\textbf{Impact of Perfect Input}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
\textbf{Model} & \textbf{Pipeline (OCR)} & \textbf{Oracle (GT)} & \textbf{Pipeline (OCR)} & \textbf{Oracle (GT)} & \textbf{$\Delta$ BLEU} & \textbf{$\Delta$ COMET} \\
\midrule
Elan bt & 0.0460 & 0.0464 & 0.6410 & 0.6428 & +0.0004 & +0.0018 \\
Qwen2.5-0.5B & 0.0536 & 0.0504 & 0.6394 & 0.6396 & -0.0032 & +0.0002 \\
\textbf{Qwen2.5-1.5B} & \textbf{0.0534} & \textbf{0.0674} & \textbf{0.6896} & \textbf{0.6855} & \textbf{+0.0140} & -0.0041 \\
\bottomrule
\end{tabular}
}
\end{table}

To quantify the impact of OCR errors on downstream translation quality, we conducted an oracle experiment (Simulated Perfect OCR). In this setup, we fed the ground-truth Japanese text directly into the translation models, bypassing the OCR stage entirely. Comparing these "Oracle" results with the actual "End-to-End" pipeline results reveals the sensitivity of each model to input noise.
While the NMT baseline (Elan) and the smaller LLM (Qwen2.5-0.5B) showed almost no performance change with perfect inputs, the larger Qwen2.5-1.5B model demonstrated a significant boost in lexical precision, increasing its BLEU score from 0.0534 to 0.0674 (+26\% relative improvement).
Moreover, the semantic understanding (measured by COMET) remained stable across all models, with small differences between the noisy OCR input and perfect Ground Truth input. This indicates that modern LLMs are highly resilient to the error generated by the OCR model, they can successfully infer the correct meaning of a sentence even when the input text contains minor recognition errors.



